---
title: "HW6"
author: "Wes Bonelli"
output:
  pdf_document: default
  html_notebook: default
---

## 1.

```{r}
proficiency = read.table('proficiency.txt', header = T, sep = '')
proficiency$TM <- as.factor(proficiency$TM)
proficiency_no_tm = proficiency[1:length(proficiency)-1]
```

### 1a.

```{r}
stem(proficiency_no_tm$E1)
stem(proficiency_no_tm$E2)
stem(proficiency_no_tm$E3)
stem(proficiency_no_tm$E4)
```

From the stem-and-leaf plots we see that E1, E2, and E3 are unimodal while E4 appears to be bimodal. E1 and E3 seem roughly normal while E2 skews negative.

### 1b.

```{r}
pairs(proficiency_no_tm)
library(corrplot)
proficiency_no_tm_cor = cor(proficiency_no_tm)
corrplot(proficiency_no_tm_cor)
```
E3 and E4 scores show a strong positive correlation with job performance rating, and are also correlated with one another, though it's unclear whether the correlation is large enough to produce collinearity problems.

### 1c.

```{r}
proficiency_model_no_tm = lm(JPR ~ E1 + E2 + E3 + E4, data = proficiency_no_tm)
summary(proficiency_model_no_tm)
```

### 1d.

As can be seen from the model summary, E1, E3, and E4 are significant terms, while E2 is just above the $\alpha$ = 0.05 significance threshold.

```{r}
library(car)
vif(proficiency_model_no_tm)
```

As observed above, no pairs of independent variables show enough collinearity to provoke serious concern (variance inflation factors are all less than 10). Thus the best model is likely:

```{r}
proficiency_model_no_tm_3 = lm(JPR ~ E1 + E3 + E4, data = proficiency_no_tm)
summary(proficiency_model_no_tm_3)
```

To confirm:

```{r}
library(leaps)
leaps(
  x=proficiency_no_tm[,c(2,3,4,5)],
  y=proficiency_no_tm[,1],
  names=names(proficiency_no_tm)[c(2,3,4,5)],
  method="r2",
  nbest=1)
anova(proficiency_model_no_tm, proficiency_model_no_tm_3)
```

The reduction F-test returns a p-value of 0.05106; at a $\alpha = 0.05$ level, there is almost justification to include E2 and use the full model, but the best model is likely still the 3-parameter model.

### 1e.

```{r}
library('ggplot2')
proficiency_coefficients = coef(proficiency_model_no_tm_3)
proficiency_no_tm$predicted = predict(proficiency_model_no_tm_3)
proficiency_no_tm$residuals = residuals(proficiency_model_no_tm_3)
ggplot(proficiency_no_tm, aes(x = E1, y = JPR)) +
  geom_segment(aes(xend = E1, yend = predicted), alpha = .2) +
  geom_point(aes(size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency_no_tm, aes(x = E2, y = JPR)) +
  geom_segment(aes(xend = E2, yend = predicted), alpha = .2) +
  geom_point(aes(size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency_no_tm, aes(x = E3, y = JPR)) +
  geom_segment(aes(xend = E3, yend = predicted), alpha = .2) +
  geom_point(aes(size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency_no_tm, aes(x = E4, y = JPR)) +
  geom_segment(aes(xend = E4, yend = predicted), alpha = .2) +
  geom_point(aes(size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
library(magrittr)
library(tidyr)
proficiency_no_tm %>% 
  gather(key = "iv", value = "score", -JPR, -predicted, -residuals) %>%
  ggplot(aes(x = score, y = JPR)) +
  geom_segment(aes(xend = score, yend = predicted), alpha = .2) +
  geom_point(aes(size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(rows = vars(iv), scales = 'fixed', as.table = TRUE) +
  theme_bw()
```

Applicants with marginally higher scores than predicted are show in lighter blue, while those with marginally lower scores are shown in darker blue. Residual magnitude is indicated by the size of point.

### 1f.

```{r}
proficiency_model = lm(JPR ~ E1 + E2 + E3 + E4 + factor(TM), data = proficiency)
summary(proficiency_model)
proficiency_coefficients = coef(proficiency_model)
proficiency$predicted = predict(proficiency_model)
proficiency$residuals = residuals(proficiency_model)
ggplot(proficiency, aes(x = E1, y = JPR)) +
  geom_segment(aes(xend = E1, yend = predicted), alpha = .2) +
  geom_point(aes(shape = TM, size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency, aes(x = E2, y = JPR)) +
  geom_segment(aes(xend = E2, yend = predicted), alpha = .2) +
  geom_point(aes(shape = TM, size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency, aes(x = E3, y = JPR)) +
  geom_segment(aes(xend = E3, yend = predicted), alpha = .2) +
  geom_point(aes(shape = TM, size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(proficiency, aes(x = E4, y = JPR)) +
  geom_segment(aes(xend = E4, yend = predicted), alpha = .2) +
  geom_point(aes(shape = TM, size = abs(residuals), color = residuals, alpha = .3)) +
  guides(color = FALSE, size = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
library(magrittr)
library(tidyr)
proficiency %>% 
  gather(key = "iv", value = "score", -JPR, -predicted, -residuals, -TM) %>%
  ggplot(aes(x = score, y = JPR)) +
  geom_segment(aes(xend = score, yend = predicted), alpha = .2) +
  geom_point(aes(shape = TM, size = abs(residuals), alpha = .3)) +
  guides(color = FALSE, size = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  facet_grid(rows = vars(iv), scales = 'fixed', as.table = TRUE) +
  theme_bw()
```

Applicants with marginally higher scores than predicted are show in lighter blue, while those with marginally lower scores are shown in darker blue. Residual magnitude is indicated by the size of point. Time of hiring is indicated by shape. From the model summary we can see that time of hiring is associated with a p-value of 0.7618, thus there is not evidence that TM affects JPR. This is reflected in the plots: circles and triangles seem approximately equally distributed with respect to the vertical axis.

## 2.

```{r}
gradrec = read.table('gradrec.txt', header = T, sep = '')
```

### 2a.

```{r}
gradrec$IP = ifelse(gradrec$DG == 'P', 1, 0)
gradrec$IF = ifelse(gradrec$SX == 'F', 1, 0)
gradrec_model = lm(SCORE ~ E1 + E2 + GPA + IP + IF, data = gradrec)
summary(gradrec_model)
vif(gradrec_model)
gradrec_coefficients = coef(gradrec_model)
gradrec$predicted = predict(gradrec_model)
gradrec$residuals = residuals(gradrec_model)
ggplot(gradrec, aes(x = E1, y = SCORE)) +
  geom_segment(aes(xend = E1, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = E2, y = SCORE)) +
  geom_segment(aes(xend = E2, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = GPA, y = SCORE)) +
  geom_segment(aes(xend = GPA, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = IP, y = SCORE)) +
  geom_segment(aes(xend = IP, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = IF, y = SCORE)) +
  geom_segment(aes(xend = IF, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
```

None of the variables are sufficiently collinear to cause concern, and the clear linear trends in the predictors E1, E2, and GPA suggests that a polynomial model is unnecessary. The full model with no additional polynomial terms is likely the best if we must use all 5 predictors.

This model's $R^2$ = 0.9577.

### 2b.

```{r}
predict(
  gradrec_model,
  newdata = data.frame(E1=c(75), E2=c(69), GPA=c(3.75), IP=c(0), IF=c(1)),
  interval = 'confidence',
  level = 0.95)
```

The 95% confidence interval is approx. [73.3623, 75.7698].

### 2c.

```{r}
predict(
  gradrec_model,
  newdata = data.frame(E1=c(75), E2=c(69), GPA=c(3.75), IP=c(0), IF=c(1)),
  interval = 'confidence',
  level = 0.90)
```

The 90% confidence interval is approx. [73.561, 75.571].

### 2d.

As seen above, GPA and scores on Exams 1 and 2 are significantly associated with overall score. Master's/PhD status and sex, on the other hand, can be discarded at $\alpha = 0.1$, even though degree status does seem to be slightly associated with score. None of the variables are sufficiently collinear to cause concern, and the clear linear trends in the other predictors suggests that a polynomial model is unnecessary. The best model is likely:

```{r}
gradrec_model_best = lm(SCORE ~ E1 + E2 + GPA, data = gradrec)
summary(gradrec_model_best)
```

To confirm:

```{r}
library(leaps)
leaps(
  x=gradrec[,c(2,3,4,8,9)],
  y=gradrec[,7],
  names=names(gradrec)[c(2,3,4,8,9)],
  method="r2",
  nbest=1)
```

As expected, the best regression equation to use with a $\alpha = 0.10$ threshold includes E1, E2, and GPA. Performing the F-Test:

```{r}
anova(gradrec_model, gradrec_model_best)
```

The p-value is 0.3338, indicating that the 5-parameter model is not a significantly better fit than the 3-parameter model.

### 2e.

```{r}
gradrec_coefficients_best = coef(gradrec_model_best)
gradrec$predicted_best = predict(gradrec_model_best)
gradrec$residuals_best = residuals(gradrec_model_best)
ggplot(gradrec, aes(x = E1, y = SCORE)) +
  geom_segment(aes(xend = E1, yend = predicted_best), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted_best), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = E2, y = SCORE)) +
  geom_segment(aes(xend = E2, yend = predicted_best), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted_best), shape = 1) +
  theme_bw()
ggplot(gradrec, aes(x = GPA, y = SCORE)) +
  geom_segment(aes(xend = GPA, yend = predicted_best), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted_best), shape = 1) +
  theme_bw()
```

All 3 predictors show a strong positive (roughly) linear correlation with score, although E1 scores (as well as predictions) vary more than E2 and GPA. Note also the small cluster of GPA values at 4.0.

### 2f.

```{r}
table(gradrec[5:6])
```

In this department it seems male and female students are about equally likely to go on to a PhD.

### 2g.

The models are so different because exam scores and GPA predict student rating much better than MS/PhD status. Degree status can explain a modest amount of student score (i.e., it has a small effect on the multiple regression model's slope), but contributes fairly little in combination with E1, E2, and GPA.

## 3.

```{r}
gasd = read.table('gasd96.txt', sep = '', header = T)
gasd = gasd[2:length(gasd)]
press <- function(linear.model) { # thanks Rawane!
  return(sum(linear.model$residuals/(1-hatvalues(linear.model))))
}
```

### 3a.

```{r}
gasd_cor = cor(gasd[, sapply(gasd, is.numeric)])
gasd_cor
corrplot(gasd_cor)
```

Most observed correlations behave as expected. T3 and T5 scores both show strong positive correlations with T8 score (about 0.77 and 0.82, respectively). Small schools  do not seem to be associated with the best student scores, nor do medium-sized schools; correlations with T3, T5, and T8 scores are slightly stronger for large-enrollment districts than medium-enrollment, and slightly larger still for metropolitan areas, while population density is moderately positively correlated with higher scores. This seems to provide the least counter-evidence for the hypothesis that education tends to be better in urban areas, but this remains speculative. PLUN shows a strong negative correlation with all three test scores, as expected.

### 3b.

Only the pair of T3 and T5 would be disallowed.

### 3c.

```{r}
gasd_model_full = lm(T8 ~ T3 + T5 + ENRL + DENS + IMTR + IM + IL + PLUN, data = gasd)
summary(gasd_model_full)
anova(gasd_model_full)
paste("PRESS:", press(gasd_model_full))
paste("AIC:", extractAIC(gasd_model_full, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_full, k = log(nrow(gasd)))[2])
gasd$predicted = predict(gasd_model_full)
gasd$residuals = residuals(gasd_model_full)
ggplot(gasd, aes(x = T3, y = T8)) +
  geom_segment(aes(xend = T3, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = T5, y = T8)) +
  geom_segment(aes(xend = T5, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = ENRL, y = T8)) +
  geom_segment(aes(xend = ENRL, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = DENS, y = T8)) +
  geom_segment(aes(xend = DENS, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = IMTR, y = T8)) +
  geom_segment(aes(xend = IMTR, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = IM, y = T8)) +
  geom_segment(aes(xend = IM, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = IL, y = T8)) +
  geom_segment(aes(xend = IL, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
ggplot(gasd, aes(x = PLUN, y = T8)) +
  geom_segment(aes(xend = PLUN, yend = predicted), alpha = .2) +
  geom_point(aes(color = residuals)) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE, size = FALSE, shape = FALSE, alpha = FALSE) +
  geom_point(aes(y = predicted), shape = 1) +
  theme_bw()
```

| Selection       | Variables Included     | Prm     |df     |$R^2$     |SSE     |RMSE     |RMSP     |AIC     |BIC     |
| :-------------- | :----------: | -----------: |-----------: |-----------: |-----------: |-----------: |-----------: |-----------: |-----------: |
| Backwards   |                  |             |            |            |            |            |            |            |            |
| Null Model    | (Intercept Only) | 1 | 173           | 0.0000           | 57039           | 18.16           | 18.21           | 1010           |  1013          |
|Best 1 Var        |            |2    |            |            |            |            |            |            |            |
|Best 2 Var        |            |3    |            |            |            |            |            |            |            |
|Best 3 Var        |            |4    |            |            |            |            |            |            |            |
|Best 4 Var        |            |5    |            |            |            |            |            |            |            |
|Best 5 Var        |            |6    |            |            |            |            |            |            |            |
|Best 6 Var        |            |7    |            |            |            |            |            |            |            |
|Best 7 Var        |            |8    |            |            |            |            |            |            |            |
|Full Model        |All 8 X's included    |9    | 165         | 0.7595    | 83.1379  | 9.118  | 3.133          | 777.908           | 806.339           |

### 3d.

From the above model summary, we can see that only T3, T5, and PLUN have p-values less than $\alpha = 0.01$. Using backwards selection, the best model is:

```{r}
gasd_model_backwards_best = lm(T8 ~ T3 + T5 + PLUN, data = gasd)
summary(gasd_model_backwards_best)
anova(gasd_model_backwards_best)
paste("PRESS:", press(gasd_model_backwards_best))
paste("AIC:", extractAIC(gasd_model_backwards_best, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_backwards_best, k = log(nrow(gasd)))[2])
```

With $\alpha = 0.05$, DENS would also have been included.

### 3e.

No, none of the predictor variables excluded from the model from 3d have p-values below 0.01.

### 3f.

```{r}
leaps(
  x=gasd[,c(3,4,5,6,7,8,9,10)],
  y=gasd[,2],
  names=names(gasd)[c(3,4,5,6,7,8,9,10)],
  method="r2",
  nbest=1)
paste("-------------------- 7-variable --------------------")
gasd_model_7 = lm(T8 ~ T3 + T5 + ENRL + DENS + IMTR + IM + PLUN, data = gasd)
summary(gasd_model_7)
anova(gasd_model_7)
paste("PRESS:", press(gasd_model_7))
paste("AIC:", extractAIC(gasd_model_7, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_7, k = log(nrow(gasd)))[2])
paste("-------------------- 6-variable --------------------")
gasd_model_6 = lm(T8 ~ T3 + T5 + DENS + IM + IL + PLUN, data = gasd)
summary(gasd_model_6)
anova(gasd_model_6)
paste("PRESS:", press(gasd_model_6))
paste("AIC:", extractAIC(gasd_model_6, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_6, k = log(nrow(gasd)))[2])
paste("-------------------- 5-variable --------------------")
gasd_model_5 = lm(T8 ~ T3 + T5 + IM + IL + PLUN, data = gasd)
summary(gasd_model_5)
anova(gasd_model_5)
paste("PRESS:", press(gasd_model_5))
paste("AIC:", extractAIC(gasd_model_5, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_5, k = log(nrow(gasd)))[2])
paste("-------------------- 4-variable --------------------")
gasd_model_4 = lm(T8 ~ T3 + T5 + IM + PLUN, data = gasd)
summary(gasd_model_4)
anova(gasd_model_4)
paste("PRESS:", press(gasd_model_4))
paste("AIC:", extractAIC(gasd_model_4, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_4, k = log(nrow(gasd)))[2])
paste("-------------------- 3-variable --------------------")
gasd_model_3 = lm(T8 ~ T3 + T5 + PLUN, data = gasd)
summary(gasd_model_3)
anova(gasd_model_3)
paste("PRESS:", press(gasd_model_3))
paste("AIC:", extractAIC(gasd_model_3, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_3, k = log(nrow(gasd)))[2])
paste("-------------------- 2-variable --------------------")
gasd_model_2 = lm(T8 ~ T5 + PLUN, data = gasd)
summary(gasd_model_2)
anova(gasd_model_2)
paste("PRESS:", press(gasd_model_2))
paste("AIC:", extractAIC(gasd_model_2, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_2, k = log(nrow(gasd)))[2])
paste("-------------------- 1-variable --------------------")
gasd_model_1 = lm(T8 ~ T5, data = gasd)
summary(gasd_model_1)
anova(gasd_model_1)
paste("PRESS:", press(gasd_model_1))
paste("AIC:", extractAIC(gasd_model_1, k = 2)[2])
paste("BIC:", extractAIC(gasd_model_1, k = log(nrow(gasd)))[2])
```


| Selection       | Variables Included     | Prm     |df     |$R^2$     |SSE     |RMSE     |RMSP     |AIC     |BIC     |
| :-------------- | :----------: | -----------: |-----------: |-----------: |-----------: |-----------: |-----------: |-----------: |-----------: |
| Backwards   | T3, T5, PLUN                 | 4            | 170           |  0.7364          | 15037           | 9.405           | 1.239           | 783.907           | 796.543           |
| Null Model    | (Intercept Only) | 1 | 173           | 0.0000           | 57039           | 18.16           | 18.21           | 1010           |  1013          |
|Best 1 Var        |T5            |2    | 172           | 0.6696           | 18847           | 10.47           | 2.647           | 819.197           | 825.516           |
|Best 2 Var        |T5, PLUN            |3    | 171           | 0.7177           | 16101           | 9.704            | 2.359           | 793.800           | 803.277           |
|Best 3 Var        |T3, T5, PLUN            |4    |  170           | 0.7364           | 15037           | 9.405           | 1.239           | 783.907           | 796.543           |
|Best 4 Var        |T3, T5, IM, PLUN            |5    | 169           | 0.7444           | 14580           | 9.288           | 0.018           | 780.533           | 796.328           |
|Best 5 Var        |T3, T5, IM, IL PLUN            |6    | 168           | 0.7505           | 14232           | 9.204           | -0.986           | 778.325           | 797.279           |
|Best 6 Var        |T3, T5, DENS, IL, PLUN            |7    | 167           | 0.7548           | 13984           | 9.151           | 1.054           | 777.273           | 799.386           |
|Best 7 Var        |T3, T5, ENRL, DENS, IMTR, IM, PLUN            |8    |    166           | 0.7587 | 13765           | 9.106           | 3.428           | 776.524           | 801.796           |
|Full Model        |All 8 X's included    |9    | 165         | 0.7595    | 13716  | 9.118  | 3.133          | 777.908           | 806.339           |

### 3g.

```{r}
paste('2-var - 1-var =', 0.7177 - 0.6696)
paste('3-var - 2-var =', 0.7364 - 0.7177)
paste('4-var - 3-var =', 0.7444 - 0.7364)
paste('5-var - 4-var =', 0.7505 - 0.7444)
paste('6-var - 5-var =', 0.7548 - 0.7505)
```

The 5-variable model would be selected as best.

### 3h.

```{r}
anova(gasd_model_6, gasd_model_3)
```

With a p-value of 0.006847, we can conclude a the $\alpha = 0.1$ level that a reduction from the 6-variable model to the 3-variable model is not permissible.

### 3i.

The 4-parameter model is best if we select by BIC, while the 7-variable model is chosen if we select by AIC.

### 3j.

Selecting by minimum RMSP, we should choose the 5-variable model, although the 4-variable model (as selected by BIC) is a close second. Selecting by minimum RMSE, we are led to pick the 7-variable model, like with AIC. These seem to be reasonable choices, although the simpler models may be preferable in many cases.